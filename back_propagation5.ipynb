{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import keras\n",
    "from keras.api.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークの構造\n",
    "input_size = 784    # 入力層のニューロン数\n",
    "hidden_size_1 = 256   # 隠れ層のニューロン数\n",
    "hidden_size_2 = 128   # 隠れ層のニューロン数\n",
    "hidden_size_3 = 32   # 隠れ層のニューロン数\n",
    "output_size = 10   # 出力層のニューロン数\n",
    "\n",
    "# 学習率\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.76405235,  0.40015721,  0.97873798, ..., -0.31932842,\n",
      "         0.69153875,  0.69474914],\n",
      "       [-0.72559738, -1.38336396, -1.5829384 , ...,  1.30142807,\n",
      "         0.89526027,  1.37496407],\n",
      "       [-1.33221165, -1.96862469, -0.66005632, ..., -1.10290621,\n",
      "        -0.10169727,  0.01927938],\n",
      "       ...,\n",
      "       [ 1.86140699, -0.78053759,  0.03168013, ..., -1.06306082,\n",
      "         0.43608227, -1.17683911],\n",
      "       [ 1.39020734, -0.51657398, -0.13652254, ...,  0.87964912,\n",
      "         0.80342249, -0.29504386],\n",
      "       [ 1.57119338, -0.15435302,  0.08733296, ..., -0.26611561,\n",
      "         2.52287972,  0.73131543]])\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "array([[-1.9282086 , -0.71324613, -1.33191318, ..., -0.72010946,\n",
      "        -1.48493602, -0.24299854],\n",
      "       [ 0.32642035, -1.46426994,  1.80678594, ..., -2.14202626,\n",
      "        -0.07737407,  0.99832167],\n",
      "       [-0.0237149 , -0.18364778,  0.08337452, ..., -0.89482477,\n",
      "        -0.46062788, -0.68079904],\n",
      "       ...,\n",
      "       [ 0.8853065 ,  0.29953726,  1.3869754 , ...,  0.69883284,\n",
      "         0.70857088, -0.44427252],\n",
      "       [ 2.68588811, -0.60351958, -1.0759598 , ...,  0.0863546 ,\n",
      "        -2.12239666, -0.72023286],\n",
      "       [ 0.59356011,  1.37512204,  0.01162945, ..., -1.2437044 ,\n",
      "         0.69462324,  1.00090403]])\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "array([[ 0.70160057,  0.89179469,  0.56794186, ..., -0.24572909,\n",
      "        -0.21697295, -0.86015476],\n",
      "       [ 0.46003788, -0.37665697, -0.88350759, ...,  1.30055383,\n",
      "        -0.25957956, -0.93991886],\n",
      "       [-0.77198988,  0.54558191, -0.9822063 , ...,  0.58930565,\n",
      "         0.89192403, -0.33649019],\n",
      "       ...,\n",
      "       [-0.40410862, -0.68216543, -0.09189863, ...,  0.42177041,\n",
      "        -0.58939561, -0.80040538],\n",
      "       [-0.82942902,  0.52744658,  1.77389414, ..., -1.0486911 ,\n",
      "        -0.1256656 , -1.23693837],\n",
      "       [ 0.07007951,  1.7546152 , -0.8693303 , ..., -0.44539368,\n",
      "         0.30207099, -1.99506411]])\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "array([[-0.94618201,  1.80917703,  0.91649063,  0.68162929, -1.13054136,\n",
      "        -0.55584721,  0.28494138,  1.7636944 ,  0.60972045,  1.44065968],\n",
      "       [-1.25003521,  1.04945258, -0.58662162, -0.66726388,  2.09891039,\n",
      "        -0.61039652, -0.05187037, -0.54628041, -0.54789789, -1.23783509],\n",
      "       [ 0.31884186, -0.57416681,  0.15420219,  1.98530252, -0.77352919,\n",
      "         0.01623487,  0.51599919,  0.1653351 ,  0.41902565,  0.49361177],\n",
      "       [-0.37255549,  1.32466277, -1.88826679,  1.3811083 , -1.43010912,\n",
      "         0.09357207,  1.69632378, -0.68838047,  1.20981373,  0.10621469],\n",
      "       [-1.15803512,  0.03969344, -0.41450136,  0.66810291, -0.92026348,\n",
      "         2.17596249,  0.27560479, -0.10076947, -1.69979677,  1.19192249],\n",
      "       [ 0.257149  ,  0.45932383,  1.47376966, -0.59548659,  1.38382319,\n",
      "         1.30085385,  0.23813975,  0.59736871, -0.69638012, -1.40263249],\n",
      "       [ 1.27216608, -0.77507064, -0.25612576, -0.60799142, -0.79361037,\n",
      "         0.0168972 ,  0.44263978,  0.04660309,  0.06871414,  1.0872948 ],\n",
      "       [ 1.33130081, -0.30917107, -0.32973681,  0.09736292,  0.69668202,\n",
      "         0.43382125,  0.87427835, -0.87841698, -0.94938278, -1.90540408],\n",
      "       [ 1.17153749,  0.39117763,  0.90281105, -0.81628965, -2.06557309,\n",
      "        -2.08138024,  1.81661078, -0.43419547,  0.82915469,  0.70672899],\n",
      "       [ 0.52821009,  2.25440271, -1.27990898, -0.64166037, -0.17824045,\n",
      "        -0.09709637, -0.63102931,  0.49767094,  0.68873881, -2.5486426 ],\n",
      "       [ 0.96823259, -0.27938534,  2.28242652, -1.8746205 , -0.06477166,\n",
      "        -0.40845659, -0.57625075,  1.61077226,  1.18941385,  1.40727777],\n",
      "       [ 0.36677558, -0.1891895 , -0.16593585,  1.25084893,  0.53255108,\n",
      "        -0.9974385 ,  0.80199638,  0.22797696,  0.30365024,  0.49693068],\n",
      "       [-0.03442047,  1.05996276,  1.39600254, -0.0488678 , -0.95611973,\n",
      "        -0.00546595,  1.99201723, -0.9426447 ,  1.44981949,  2.01516239],\n",
      "       [ 0.66758722, -1.26151261, -1.12799049, -0.00381391, -0.84245268,\n",
      "         0.0079131 ,  1.0031707 ,  0.86762163, -0.13942332,  1.01504978],\n",
      "       [-0.88022733,  0.44085701, -0.65621109,  0.53381086,  0.0692993 ,\n",
      "         0.60326299, -0.31466757, -1.27507133,  1.10010645,  1.86481514],\n",
      "       [ 0.44368138, -1.23625252, -0.16514433,  1.60106308,  1.28822406,\n",
      "         1.22799767,  1.07566904, -0.79535899, -0.93605058, -1.01227403],\n",
      "       [-0.18644134, -0.3098149 , -1.04222089,  0.917522  ,  0.27130412,\n",
      "        -1.12980756,  0.1122226 ,  1.13680856,  0.13830395, -1.09139489],\n",
      "       [-0.08022523,  0.1770742 ,  1.68189554,  0.55294954, -1.07825652,\n",
      "         0.72958072,  0.94831183,  2.73030343, -0.91332222,  1.918463  ],\n",
      "       [-0.85800119, -0.77262445, -1.12062483, -0.45930874, -0.17562735,\n",
      "        -0.34521242,  0.18885422,  0.95789941,  0.14786973, -1.31993868],\n",
      "       [-1.46550763,  0.05188128,  0.76025704, -0.61040129,  0.41335609,\n",
      "        -1.32667937,  0.03996124, -0.45724388,  1.69013359,  0.8381504 ],\n",
      "       [-0.49345526,  1.41893763, -0.20255631, -0.0397165 ,  1.12691042,\n",
      "         2.62061259, -1.3395178 ,  0.02077595,  1.89682655, -0.63771994],\n",
      "       [-0.33725711,  0.25107642, -0.66654889, -0.44857016, -0.09106201,\n",
      "         0.54501127, -0.16997543, -1.34034189, -0.22191953,  1.53504857],\n",
      "       [-1.54924908,  1.27435048, -0.26191403,  0.74897581,  0.90725936,\n",
      "         0.10723607,  0.51655605, -1.97192596, -0.97746866,  1.75765789],\n",
      "       [-0.71000389,  1.02636983, -0.52647504,  0.40862313,  0.38786097,\n",
      "        -0.33453465,  0.28503026, -0.84873803, -0.07236602, -0.03001313],\n",
      "       [ 0.87442002,  1.13806258,  0.96232368, -1.17466581,  0.51307156,\n",
      "         0.09600771, -0.08396156,  1.08016278,  0.82354185,  0.01656155],\n",
      "       [-0.87269442, -1.14058829,  0.86529504,  0.58016956,  0.31604277,\n",
      "        -1.01401837,  0.73615946, -0.55042238,  0.5401441 ,  0.9894795 ],\n",
      "       [-1.77493818,  1.02602921,  0.11739689, -1.51198614,  0.44605494,\n",
      "         0.53910474,  0.17053578, -0.53019236, -0.46325342,  0.2024134 ],\n",
      "       [ 0.52465342, -0.43684091,  0.08607695,  0.31217126,  0.89702279,\n",
      "        -0.01776314,  0.25514448,  0.32066219, -1.90136709,  0.03350087],\n",
      "       [ 0.23214112, -0.93331607,  1.05902278, -1.04974977, -1.08111784,\n",
      "         0.52494719, -0.99367632,  0.3897748 , -0.84724346, -0.76113182],\n",
      "       [ 0.26698046,  0.64880218,  1.11944665, -1.73859444, -0.06306303,\n",
      "         0.24544447, -0.28956564, -1.24017505,  1.33121237,  0.36683408],\n",
      "       [-0.61186583, -0.05647817,  1.92662093, -0.06099873, -1.66271264,\n",
      "        -0.42921893,  1.86476794,  0.74073796,  1.8064427 , -1.79083259],\n",
      "       [-0.9738545 , -0.26099143,  0.39394874, -0.95281126, -1.5444451 ,\n",
      "         1.19258641, -0.79625044,  1.06819908, -0.83716855, -0.33502588]])\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 重みの初期化\n",
    "np.random.seed(0) # 再現性のためのシード値設定\n",
    "W1 = np.random.randn(input_size, hidden_size_1)  # 入力層から隠れ層1への重み\n",
    "b1 = np.zeros((1, hidden_size_1))                # 隠れ層1のバイアス\n",
    "W2 = np.random.randn(hidden_size_1, hidden_size_2) # 隠れ層1から隠れ層2への重み\n",
    "b2 = np.zeros((1, hidden_size_2))                # 隠れ層2のバイアス\n",
    "W3 = np.random.randn(hidden_size_2, hidden_size_3) # 隠れ層2から出力層への重み\n",
    "b3 = np.zeros((1, hidden_size_3))                # 出力層のバイアス\n",
    "W4 = np.random.randn(hidden_size_3, output_size) # 隠れ層2から出力層への重み\n",
    "b4 = np.zeros((1, output_size))                # 出力層のバイアス\n",
    "\n",
    "pprint.pprint(W1)\n",
    "pprint.pprint(b1)\n",
    "pprint.pprint(W2)\n",
    "pprint.pprint(b2)\n",
    "pprint.pprint(W3)\n",
    "pprint.pprint(b3)\n",
    "pprint.pprint(W4)\n",
    "pprint.pprint(b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# シグモイド関数の導関数\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# relu関数\n",
    "def relu(x):\n",
    "    return x * (0 < x)\n",
    "\n",
    "# relu関数の導関数\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X):\n",
    "    # 隠れ層1への入力\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # 隠れ層2への入力\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 隠れ層3への入力\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # 出力層への入力\n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "    \n",
    "    return a1, a2, a3, a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, y, a1, a2, a3, a4):\n",
    "    global W1, b1, W2, b2, W3, b3, W4, b4\n",
    "\n",
    "    # 出力層の誤差\n",
    "    output_error = y - a4\n",
    "    output_delta = output_error * sigmoid_derivative(a4)\n",
    "\n",
    "    # 隠れ層の誤差\n",
    "    hidden_3_error = np.dot(output_delta, W4.T)\n",
    "    hidden_3_delta = hidden_3_error * sigmoid_derivative(a3)\n",
    "\n",
    "    # 隠れ層の誤差\n",
    "    hidden_2_error = np.dot(hidden_3_delta, W3.T)\n",
    "    hidden_2_delta = hidden_2_error * sigmoid_derivative(a2)\n",
    "\n",
    "    # 隠れ層の誤差\n",
    "    hidden_1_error = np.dot(hidden_2_delta, W2.T)\n",
    "    hidden_1_delta = hidden_1_error * sigmoid_derivative(a1)\n",
    "\n",
    "    # 重みとバイアスの更新\n",
    "    W4 += learning_rate * np.dot(a3.T, output_delta)\n",
    "    b4 += learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "    W3 += learning_rate * np.dot(a2.T, hidden_3_delta)\n",
    "    b3 += learning_rate * np.sum(hidden_3_delta, axis=0, keepdims=True)\n",
    "    W2 += learning_rate * np.dot(a1.T, hidden_2_delta)\n",
    "    b2 += learning_rate * np.sum(hidden_2_delta, axis=0, keepdims=True)\n",
    "    W1 += learning_rate * np.dot(X.T, hidden_1_delta)\n",
    "    b1 += learning_rate * np.sum(hidden_1_delta, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, iterations):\n",
    "    for i in range(iterations):\n",
    "        # フォワードプロパゲーション\n",
    "        a1, a2, a3, a4 = forward_propagation(X)\n",
    "        \n",
    "        # バックプロパゲーション\n",
    "        backward_propagation(X, y, a1, a2, a3, a4)\n",
    "        \n",
    "        if iterations <= 300:\n",
    "            loss = np.mean(np.square(y - a4))\n",
    "            print(f'Iteration {i+1}, Loss: {loss}')\n",
    "            continue\n",
    "            \n",
    "        if (i+1) % 1000 == 0:\n",
    "            loss = np.mean(np.square(y - a4))\n",
    "            print(f'Iteration {i+1}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Iteration 1, Loss: 0.5150257601150099\n",
      "Iteration 2, Loss: 0.09977476952473072\n",
      "Iteration 3, Loss: 0.17985125346467112\n",
      "Iteration 4, Loss: 0.09999999948454616\n",
      "Iteration 5, Loss: 0.0999999994845334\n",
      "Iteration 6, Loss: 0.09999999948452072\n",
      "Iteration 7, Loss: 0.09999999948450804\n",
      "Iteration 8, Loss: 0.09999999948449534\n",
      "Iteration 9, Loss: 0.09999999948448257\n",
      "Iteration 10, Loss: 0.09999999948446991\n",
      "Iteration 11, Loss: 0.09999999948445717\n",
      "Iteration 12, Loss: 0.09999999948444445\n",
      "Iteration 13, Loss: 0.0999999994844318\n",
      "Iteration 14, Loss: 0.09999999948441912\n",
      "Iteration 15, Loss: 0.09999999948440638\n",
      "Iteration 16, Loss: 0.09999999948439363\n",
      "Iteration 17, Loss: 0.09999999948438096\n",
      "Iteration 18, Loss: 0.09999999948436825\n",
      "Iteration 19, Loss: 0.09999999948435552\n",
      "Iteration 20, Loss: 0.09999999948434285\n",
      "Iteration 21, Loss: 0.09999999948433017\n",
      "Iteration 22, Loss: 0.09999999948431741\n",
      "Iteration 23, Loss: 0.0999999994843047\n",
      "Iteration 24, Loss: 0.09999999948429202\n",
      "Iteration 25, Loss: 0.09999999948427929\n",
      "Iteration 26, Loss: 0.0999999994842666\n",
      "Iteration 27, Loss: 0.09999999948425388\n",
      "Iteration 28, Loss: 0.09999999948424117\n",
      "Iteration 29, Loss: 0.09999999948422841\n",
      "Iteration 30, Loss: 0.0999999994842157\n",
      "Iteration 31, Loss: 0.09999999948420302\n",
      "Iteration 32, Loss: 0.09999999948419025\n",
      "Iteration 33, Loss: 0.09999999948417755\n",
      "Iteration 34, Loss: 0.09999999948416484\n",
      "Iteration 35, Loss: 0.09999999948415218\n",
      "Iteration 36, Loss: 0.0999999994841394\n",
      "Iteration 37, Loss: 0.09999999948412668\n",
      "Iteration 38, Loss: 0.09999999948411395\n",
      "Iteration 39, Loss: 0.09999999948410125\n",
      "Iteration 40, Loss: 0.09999999948408854\n",
      "Iteration 41, Loss: 0.09999999948407581\n",
      "Iteration 42, Loss: 0.09999999948406306\n",
      "Iteration 43, Loss: 0.09999999948405033\n",
      "Iteration 44, Loss: 0.09999999948403766\n",
      "Iteration 45, Loss: 0.09999999948402492\n",
      "Iteration 46, Loss: 0.0999999994840122\n",
      "Iteration 47, Loss: 0.09999999948399947\n",
      "Iteration 48, Loss: 0.09999999948398673\n",
      "Iteration 49, Loss: 0.09999999948397402\n",
      "Iteration 50, Loss: 0.09999999948396128\n",
      "Iteration 51, Loss: 0.09999999948394854\n",
      "Iteration 52, Loss: 0.09999999948393586\n",
      "Iteration 53, Loss: 0.0999999994839231\n",
      "Iteration 54, Loss: 0.09999999948391039\n",
      "Iteration 55, Loss: 0.09999999948389765\n",
      "Iteration 56, Loss: 0.09999999948388492\n",
      "Iteration 57, Loss: 0.09999999948387218\n",
      "Iteration 58, Loss: 0.09999999948385946\n",
      "Iteration 59, Loss: 0.0999999994838467\n",
      "Iteration 60, Loss: 0.09999999948383399\n",
      "Iteration 61, Loss: 0.09999999948382125\n",
      "Iteration 62, Loss: 0.09999999948380853\n",
      "Iteration 63, Loss: 0.09999999948379577\n",
      "Iteration 64, Loss: 0.09999999948378303\n",
      "Iteration 65, Loss: 0.0999999994837703\n",
      "Iteration 66, Loss: 0.09999999948375758\n",
      "Iteration 67, Loss: 0.09999999948374483\n",
      "Iteration 68, Loss: 0.09999999948373209\n",
      "Iteration 69, Loss: 0.0999999994837193\n",
      "Iteration 70, Loss: 0.09999999948370661\n",
      "Iteration 71, Loss: 0.09999999948369387\n",
      "Iteration 72, Loss: 0.09999999948368114\n",
      "Iteration 73, Loss: 0.0999999994836684\n",
      "Iteration 74, Loss: 0.09999999948365562\n",
      "Iteration 75, Loss: 0.0999999994836429\n",
      "Iteration 76, Loss: 0.09999999948363011\n",
      "Iteration 77, Loss: 0.0999999994836174\n",
      "Iteration 78, Loss: 0.09999999948360463\n",
      "Iteration 79, Loss: 0.09999999948359194\n",
      "Iteration 80, Loss: 0.09999999948357914\n",
      "Iteration 81, Loss: 0.09999999948356642\n",
      "Iteration 82, Loss: 0.09999999948355366\n",
      "Iteration 83, Loss: 0.09999999948354091\n",
      "Iteration 84, Loss: 0.09999999948352814\n",
      "Iteration 85, Loss: 0.09999999948351544\n",
      "Iteration 86, Loss: 0.09999999948350263\n",
      "Iteration 87, Loss: 0.09999999948348993\n",
      "Iteration 88, Loss: 0.09999999948347717\n",
      "Iteration 89, Loss: 0.09999999948346443\n",
      "Iteration 90, Loss: 0.09999999948345166\n",
      "Iteration 91, Loss: 0.09999999948343893\n",
      "Iteration 92, Loss: 0.09999999948342614\n",
      "Iteration 93, Loss: 0.09999999948341336\n",
      "Iteration 94, Loss: 0.09999999948340063\n",
      "Iteration 95, Loss: 0.09999999948338789\n",
      "Iteration 96, Loss: 0.09999999948337512\n",
      "Iteration 97, Loss: 0.09999999948336238\n",
      "Iteration 98, Loss: 0.09999999948334964\n",
      "Iteration 99, Loss: 0.09999999948333685\n",
      "Iteration 100, Loss: 0.09999999948332412\n",
      "Iteration 101, Loss: 0.09999999948331133\n",
      "Iteration 102, Loss: 0.0999999994832986\n",
      "Iteration 103, Loss: 0.09999999948328578\n",
      "Iteration 104, Loss: 0.09999999948327304\n",
      "Iteration 105, Loss: 0.09999999948326031\n",
      "Iteration 106, Loss: 0.09999999948324752\n",
      "Iteration 107, Loss: 0.09999999948323479\n",
      "Iteration 108, Loss: 0.09999999948322202\n",
      "Iteration 109, Loss: 0.09999999948320924\n",
      "Iteration 110, Loss: 0.09999999948319649\n",
      "Iteration 111, Loss: 0.09999999948318376\n",
      "Iteration 112, Loss: 0.09999999948317093\n",
      "Iteration 113, Loss: 0.09999999948315814\n",
      "Iteration 114, Loss: 0.09999999948314535\n",
      "Iteration 115, Loss: 0.0999999994831326\n",
      "Iteration 116, Loss: 0.09999999948311987\n",
      "Iteration 117, Loss: 0.09999999948310706\n",
      "Iteration 118, Loss: 0.09999999948309435\n",
      "Iteration 119, Loss: 0.09999999948308153\n",
      "Iteration 120, Loss: 0.09999999948306881\n",
      "Iteration 121, Loss: 0.099999999483056\n",
      "Iteration 122, Loss: 0.09999999948304322\n",
      "Iteration 123, Loss: 0.09999999948303047\n",
      "Iteration 124, Loss: 0.09999999948301766\n",
      "Iteration 125, Loss: 0.09999999948300493\n",
      "Iteration 126, Loss: 0.09999999948299212\n",
      "Iteration 127, Loss: 0.09999999948297937\n",
      "Iteration 128, Loss: 0.09999999948296658\n",
      "Iteration 129, Loss: 0.09999999948295386\n",
      "Iteration 130, Loss: 0.09999999948294101\n",
      "Iteration 131, Loss: 0.09999999948292825\n",
      "Iteration 132, Loss: 0.09999999948291548\n",
      "Iteration 133, Loss: 0.09999999948290274\n",
      "Iteration 134, Loss: 0.09999999948288993\n",
      "Iteration 135, Loss: 0.09999999948287712\n",
      "Iteration 136, Loss: 0.09999999948286434\n",
      "Iteration 137, Loss: 0.09999999948285158\n",
      "Iteration 138, Loss: 0.09999999948283875\n",
      "Iteration 139, Loss: 0.09999999948282602\n",
      "Iteration 140, Loss: 0.09999999948281317\n",
      "Iteration 141, Loss: 0.09999999948280042\n",
      "Iteration 142, Loss: 0.09999999948278769\n",
      "Iteration 143, Loss: 0.09999999948277487\n",
      "Iteration 144, Loss: 0.09999999948276203\n",
      "Iteration 145, Loss: 0.09999999948274925\n",
      "Iteration 146, Loss: 0.0999999994827365\n",
      "Iteration 147, Loss: 0.0999999994827237\n",
      "Iteration 148, Loss: 0.09999999948271092\n",
      "Iteration 149, Loss: 0.09999999948269811\n",
      "Iteration 150, Loss: 0.09999999948268536\n",
      "Iteration 151, Loss: 0.09999999948267255\n",
      "Iteration 152, Loss: 0.09999999948265974\n",
      "Iteration 153, Loss: 0.09999999948264697\n",
      "Iteration 154, Loss: 0.09999999948263417\n",
      "Iteration 155, Loss: 0.09999999948262137\n",
      "Iteration 156, Loss: 0.09999999948260856\n",
      "Iteration 157, Loss: 0.0999999994825958\n",
      "Iteration 158, Loss: 0.09999999948258297\n",
      "Iteration 159, Loss: 0.09999999948257017\n",
      "Iteration 160, Loss: 0.09999999948255738\n",
      "Iteration 161, Loss: 0.09999999948254457\n",
      "Iteration 162, Loss: 0.09999999948253184\n",
      "Iteration 163, Loss: 0.09999999948251898\n",
      "Iteration 164, Loss: 0.09999999948250618\n",
      "Iteration 165, Loss: 0.0999999994824934\n",
      "Iteration 166, Loss: 0.09999999948248063\n",
      "Iteration 167, Loss: 0.09999999948246778\n",
      "Iteration 168, Loss: 0.09999999948245498\n",
      "Iteration 169, Loss: 0.09999999948244218\n",
      "Iteration 170, Loss: 0.09999999948242945\n",
      "Iteration 171, Loss: 0.09999999948241661\n",
      "Iteration 172, Loss: 0.09999999948240373\n",
      "Iteration 173, Loss: 0.09999999948239097\n",
      "Iteration 174, Loss: 0.09999999948237817\n",
      "Iteration 175, Loss: 0.09999999948236533\n",
      "Iteration 176, Loss: 0.09999999948235253\n",
      "Iteration 177, Loss: 0.09999999948233974\n",
      "Iteration 178, Loss: 0.09999999948232695\n",
      "Iteration 179, Loss: 0.09999999948231411\n",
      "Iteration 180, Loss: 0.09999999948230133\n",
      "Iteration 181, Loss: 0.09999999948228847\n",
      "Iteration 182, Loss: 0.09999999948227571\n",
      "Iteration 183, Loss: 0.09999999948226287\n",
      "Iteration 184, Loss: 0.09999999948225007\n",
      "Iteration 185, Loss: 0.09999999948223727\n",
      "Iteration 186, Loss: 0.09999999948222445\n",
      "Iteration 187, Loss: 0.09999999948221162\n",
      "Iteration 188, Loss: 0.09999999948219884\n",
      "Iteration 189, Loss: 0.099999999482186\n",
      "Iteration 190, Loss: 0.0999999994821732\n",
      "Iteration 191, Loss: 0.09999999948216035\n",
      "Iteration 192, Loss: 0.09999999948214755\n",
      "Iteration 193, Loss: 0.09999999948213473\n",
      "Iteration 194, Loss: 0.09999999948212192\n",
      "Iteration 195, Loss: 0.09999999948210908\n",
      "Iteration 196, Loss: 0.09999999948209626\n",
      "Iteration 197, Loss: 0.09999999948208343\n",
      "Iteration 198, Loss: 0.09999999948207063\n",
      "Iteration 199, Loss: 0.09999999948205783\n",
      "Iteration 200, Loss: 0.099999999482045\n",
      "Iteration 201, Loss: 0.09999999948203217\n",
      "Iteration 202, Loss: 0.09999999948201933\n",
      "Iteration 203, Loss: 0.09999999948200655\n",
      "Iteration 204, Loss: 0.09999999948199367\n",
      "Iteration 205, Loss: 0.09999999948198085\n",
      "Iteration 206, Loss: 0.099999999481968\n",
      "Iteration 207, Loss: 0.09999999948195522\n",
      "Iteration 208, Loss: 0.09999999948194235\n",
      "Iteration 209, Loss: 0.09999999948192957\n",
      "Iteration 210, Loss: 0.09999999948191672\n",
      "Iteration 211, Loss: 0.09999999948190388\n",
      "Iteration 212, Loss: 0.09999999948189109\n",
      "Iteration 213, Loss: 0.09999999948187822\n",
      "Iteration 214, Loss: 0.0999999994818654\n",
      "Iteration 215, Loss: 0.09999999948185259\n",
      "Iteration 216, Loss: 0.09999999948183973\n",
      "Iteration 217, Loss: 0.09999999948182689\n",
      "Iteration 218, Loss: 0.09999999948181407\n",
      "Iteration 219, Loss: 0.09999999948180123\n",
      "Iteration 220, Loss: 0.09999999948178839\n",
      "Iteration 221, Loss: 0.09999999948177553\n",
      "Iteration 222, Loss: 0.09999999948176272\n",
      "Iteration 223, Loss: 0.09999999948174992\n",
      "Iteration 224, Loss: 0.09999999948173705\n",
      "Iteration 225, Loss: 0.09999999948172421\n",
      "Iteration 226, Loss: 0.09999999948171139\n",
      "Iteration 227, Loss: 0.0999999994816985\n",
      "Iteration 228, Loss: 0.09999999948168567\n",
      "Iteration 229, Loss: 0.0999999994816729\n",
      "Iteration 230, Loss: 0.09999999948166001\n",
      "Iteration 231, Loss: 0.09999999948164714\n",
      "Iteration 232, Loss: 0.09999999948163431\n",
      "Iteration 233, Loss: 0.0999999994816215\n",
      "Iteration 234, Loss: 0.0999999994816086\n",
      "Iteration 235, Loss: 0.09999999948159581\n",
      "Iteration 236, Loss: 0.09999999948158297\n",
      "Iteration 237, Loss: 0.0999999994815701\n",
      "Iteration 238, Loss: 0.09999999948155726\n",
      "Iteration 239, Loss: 0.0999999994815444\n",
      "Iteration 240, Loss: 0.09999999948153158\n",
      "Iteration 241, Loss: 0.09999999948151872\n",
      "Iteration 242, Loss: 0.09999999948150587\n",
      "Iteration 243, Loss: 0.099999999481493\n",
      "Iteration 244, Loss: 0.09999999948148018\n",
      "Iteration 245, Loss: 0.09999999948146733\n",
      "Iteration 246, Loss: 0.09999999948145448\n",
      "Iteration 247, Loss: 0.0999999994814416\n",
      "Iteration 248, Loss: 0.09999999948142878\n",
      "Iteration 249, Loss: 0.09999999948141591\n",
      "Iteration 250, Loss: 0.09999999948140302\n",
      "Iteration 251, Loss: 0.09999999948139023\n",
      "Iteration 252, Loss: 0.09999999948137733\n",
      "Iteration 253, Loss: 0.09999999948136451\n",
      "Iteration 254, Loss: 0.09999999948135162\n",
      "Iteration 255, Loss: 0.09999999948133874\n",
      "Iteration 256, Loss: 0.09999999948132589\n",
      "Iteration 257, Loss: 0.09999999948131305\n",
      "Iteration 258, Loss: 0.09999999948130023\n",
      "Iteration 259, Loss: 0.09999999948128732\n",
      "Iteration 260, Loss: 0.09999999948127447\n",
      "Iteration 261, Loss: 0.09999999948126158\n",
      "Iteration 262, Loss: 0.09999999948124877\n",
      "Iteration 263, Loss: 0.0999999994812359\n",
      "Iteration 264, Loss: 0.09999999948122307\n",
      "Iteration 265, Loss: 0.09999999948121016\n",
      "Iteration 266, Loss: 0.09999999948119728\n",
      "Iteration 267, Loss: 0.09999999948118445\n",
      "Iteration 268, Loss: 0.09999999948117153\n",
      "Iteration 269, Loss: 0.09999999948115873\n",
      "Iteration 270, Loss: 0.09999999948114587\n",
      "Iteration 271, Loss: 0.09999999948113297\n",
      "Iteration 272, Loss: 0.09999999948112008\n",
      "Iteration 273, Loss: 0.09999999948110724\n",
      "Iteration 274, Loss: 0.09999999948109432\n",
      "Iteration 275, Loss: 0.09999999948108149\n",
      "Iteration 276, Loss: 0.09999999948106861\n",
      "Iteration 277, Loss: 0.09999999948105574\n",
      "Iteration 278, Loss: 0.09999999948104288\n",
      "Iteration 279, Loss: 0.09999999948103\n",
      "Iteration 280, Loss: 0.0999999994810171\n",
      "Iteration 281, Loss: 0.09999999948100424\n",
      "Iteration 282, Loss: 0.09999999948099134\n",
      "Iteration 283, Loss: 0.09999999948097849\n",
      "Iteration 284, Loss: 0.09999999948096562\n",
      "Iteration 285, Loss: 0.09999999948095274\n",
      "Iteration 286, Loss: 0.09999999948093988\n",
      "Iteration 287, Loss: 0.099999999480927\n",
      "Iteration 288, Loss: 0.09999999948091408\n",
      "Iteration 289, Loss: 0.09999999948090127\n",
      "Iteration 290, Loss: 0.09999999948088835\n",
      "Iteration 291, Loss: 0.09999999948087548\n",
      "Iteration 292, Loss: 0.09999999948086258\n",
      "Iteration 293, Loss: 0.09999999948084971\n",
      "Iteration 294, Loss: 0.09999999948083682\n",
      "Iteration 295, Loss: 0.09999999948082394\n",
      "Iteration 296, Loss: 0.09999999948081105\n",
      "Iteration 297, Loss: 0.09999999948079819\n",
      "Iteration 298, Loss: 0.09999999948078532\n",
      "Iteration 299, Loss: 0.09999999948077241\n",
      "Iteration 300, Loss: 0.09999999948075954\n"
     ]
    }
   ],
   "source": [
    "# トレーニングデータの例\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "y = np.array([[0, 1],\n",
    "              [1, 0],\n",
    "              [1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "# データ抽出\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "num_classes = 10\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# one-hot-label に変換\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 学習を実行\n",
    "train(x_train, y_train, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "def predict(X):\n",
    "    # 隠れ層1への入力\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # 隠れ層2への入力\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 隠れ層3への入力\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # 出力層への入力\n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "    \n",
    "    return a4\n",
    "    \n",
    "    \n",
    "y_pre = predict(x_train)\n",
    "\n",
    "for i in range(5):\n",
    "    print(np.argmax(y_pre[i]), np.argmax(y_train[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
