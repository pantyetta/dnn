{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 - Loss: 2.3035\n",
      "Epoch 1/100 - Loss: 2.3012\n",
      "Epoch 2/100 - Loss: 2.3026\n",
      "Epoch 3/100 - Loss: 2.3023\n",
      "Epoch 4/100 - Loss: 2.3005\n",
      "Epoch 5/100 - Loss: 2.3011\n",
      "Epoch 6/100 - Loss: 2.3012\n",
      "Epoch 7/100 - Loss: 2.3020\n",
      "Epoch 8/100 - Loss: 2.3000\n",
      "Epoch 9/100 - Loss: 2.3017\n",
      "Epoch 10/100 - Loss: 2.2992\n",
      "Epoch 11/100 - Loss: 2.3008\n",
      "Epoch 12/100 - Loss: 2.2984\n",
      "Epoch 13/100 - Loss: 2.2996\n",
      "Epoch 14/100 - Loss: 2.2935\n",
      "Epoch 15/100 - Loss: 2.0522\n",
      "Epoch 16/100 - Loss: 1.9978\n",
      "Epoch 17/100 - Loss: 1.7849\n",
      "Epoch 18/100 - Loss: 1.4719\n",
      "Epoch 19/100 - Loss: 1.3390\n",
      "Epoch 20/100 - Loss: 1.0112\n",
      "Epoch 21/100 - Loss: 0.9341\n",
      "Epoch 22/100 - Loss: 0.7087\n",
      "Epoch 23/100 - Loss: 0.6002\n",
      "Epoch 24/100 - Loss: 0.4197\n",
      "Epoch 25/100 - Loss: 0.3024\n",
      "Epoch 26/100 - Loss: 0.2248\n",
      "Epoch 27/100 - Loss: 0.2465\n",
      "Epoch 28/100 - Loss: 0.1973\n",
      "Epoch 29/100 - Loss: 0.1507\n",
      "Epoch 30/100 - Loss: 5.2387\n",
      "Epoch 31/100 - Loss: 0.9575\n",
      "Epoch 32/100 - Loss: 0.4752\n",
      "Epoch 33/100 - Loss: 0.2435\n",
      "Epoch 34/100 - Loss: 0.2120\n",
      "Epoch 35/100 - Loss: 0.1626\n",
      "Epoch 36/100 - Loss: 0.1781\n",
      "Epoch 37/100 - Loss: 0.1274\n",
      "Epoch 38/100 - Loss: 0.1665\n",
      "Epoch 39/100 - Loss: 0.1282\n",
      "Epoch 40/100 - Loss: 0.1207\n",
      "Epoch 41/100 - Loss: 0.1466\n",
      "Epoch 42/100 - Loss: 1.1640\n",
      "Epoch 43/100 - Loss: 0.3650\n",
      "Epoch 44/100 - Loss: 0.1745\n",
      "Epoch 45/100 - Loss: 0.2454\n",
      "Epoch 46/100 - Loss: 0.1716\n",
      "Epoch 47/100 - Loss: 0.1480\n",
      "Epoch 48/100 - Loss: 0.1351\n",
      "Epoch 49/100 - Loss: 0.1269\n",
      "Epoch 50/100 - Loss: 0.1223\n",
      "Epoch 51/100 - Loss: 0.0994\n",
      "Epoch 52/100 - Loss: 0.0842\n",
      "Epoch 53/100 - Loss: 0.0927\n",
      "Epoch 54/100 - Loss: 0.0886\n",
      "Epoch 55/100 - Loss: 0.0819\n",
      "Epoch 56/100 - Loss: 0.0697\n",
      "Epoch 57/100 - Loss: 2.2248\n",
      "Epoch 58/100 - Loss: 1.7774\n",
      "Epoch 59/100 - Loss: 1.5629\n",
      "Epoch 60/100 - Loss: 0.6998\n",
      "Epoch 61/100 - Loss: 0.4296\n",
      "Epoch 62/100 - Loss: 0.2911\n",
      "Epoch 63/100 - Loss: 0.2606\n",
      "Epoch 64/100 - Loss: 0.2124\n",
      "Epoch 65/100 - Loss: 0.1867\n",
      "Epoch 66/100 - Loss: 0.1718\n",
      "Epoch 67/100 - Loss: 0.1610\n",
      "Epoch 68/100 - Loss: 0.1824\n",
      "Epoch 69/100 - Loss: 0.1557\n",
      "Epoch 70/100 - Loss: 0.1593\n",
      "Epoch 71/100 - Loss: 0.1352\n",
      "Epoch 72/100 - Loss: 0.1019\n",
      "Epoch 73/100 - Loss: 0.1052\n",
      "Epoch 74/100 - Loss: 0.1055\n",
      "Epoch 75/100 - Loss: 0.1079\n",
      "Epoch 76/100 - Loss: 0.0981\n",
      "Epoch 77/100 - Loss: 2.0447\n",
      "Epoch 78/100 - Loss: 0.9687\n",
      "Epoch 79/100 - Loss: 0.8920\n",
      "Epoch 80/100 - Loss: 0.3477\n",
      "Epoch 81/100 - Loss: 0.2339\n",
      "Epoch 82/100 - Loss: 0.2039\n",
      "Epoch 83/100 - Loss: 0.1814\n",
      "Epoch 84/100 - Loss: 0.1600\n",
      "Epoch 85/100 - Loss: 0.1299\n",
      "Epoch 86/100 - Loss: 0.1562\n",
      "Epoch 87/100 - Loss: 0.1044\n",
      "Epoch 88/100 - Loss: 0.1101\n",
      "Epoch 89/100 - Loss: 0.1254\n",
      "Epoch 90/100 - Loss: 0.1201\n",
      "Epoch 91/100 - Loss: 0.1291\n",
      "Epoch 92/100 - Loss: 0.1100\n",
      "Epoch 93/100 - Loss: 0.1937\n",
      "Epoch 94/100 - Loss: 0.1301\n",
      "Epoch 95/100 - Loss: 0.1080\n",
      "Epoch 96/100 - Loss: 0.1007\n",
      "Epoch 97/100 - Loss: 0.1184\n",
      "Epoch 98/100 - Loss: 0.1059\n",
      "Epoch 99/100 - Loss: 0.0845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.api.datasets import mnist\n",
    "from keras.api.utils import to_categorical\n",
    "\n",
    "# 活性化関数とその導関数\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    grad = y_pred.copy()\n",
    "    grad[range(m), np.argmax(y_true, axis=1)] -= 1\n",
    "    grad = grad / m\n",
    "    return grad\n",
    "\n",
    "# ネットワーククラス\n",
    "class FourLayerNet:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(input_size, hidden_size1) * 0.01,\n",
    "            'b1': np.zeros((1, hidden_size1)),\n",
    "            'W2': np.random.randn(hidden_size1, hidden_size2) * 0.01,\n",
    "            'b2': np.zeros((1, hidden_size2)),\n",
    "            'W3': np.random.randn(hidden_size2, hidden_size3) * 0.01,\n",
    "            'b3': np.zeros((1, hidden_size3)),\n",
    "            'W4': np.random.randn(hidden_size3, output_size) * 0.01,\n",
    "            'b4': np.zeros((1, output_size))\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = {}\n",
    "        self.cache['Z1'] = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        self.cache['A1'] = relu(self.cache['Z1'])\n",
    "        self.cache['Z2'] = np.dot(self.cache['A1'], self.params['W2']) + self.params['b2']\n",
    "        self.cache['A2'] = relu(self.cache['Z2'])\n",
    "        self.cache['Z3'] = np.dot(self.cache['A2'], self.params['W3']) + self.params['b3']\n",
    "        self.cache['A3'] = relu(self.cache['Z3'])\n",
    "        self.cache['Z4'] = np.dot(self.cache['A3'], self.params['W4']) + self.params['b4']\n",
    "        self.cache['A4'] = softmax(self.cache['Z4'])\n",
    "        return self.cache['A4']\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grads = {}\n",
    "\n",
    "        dZ4 = cross_entropy_loss_derivative(y, self.cache['A4'])\n",
    "        grads['dW4'] = np.dot(self.cache['A3'].T, dZ4)\n",
    "        grads['db4'] = np.sum(dZ4, axis=0, keepdims=True)\n",
    "\n",
    "        test = self.params['W4'].T\n",
    "\n",
    "        dA3 = np.dot(dZ4, self.params['W4'].T)\n",
    "        dZ3 = dA3 * relu_derivative(self.cache['Z3'])\n",
    "        grads['dW3'] = np.dot(self.cache['A2'].T, dZ3)\n",
    "        grads['db3'] = np.sum(dZ3, axis=0, keepdims=True)\n",
    "\n",
    "        dA2 = np.dot(dZ3, self.params['W3'].T)\n",
    "        dZ2 = dA2 * relu_derivative(self.cache['Z2'])\n",
    "        grads['dW2'] = np.dot(self.cache['A1'].T, dZ2)\n",
    "        grads['db2'] = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(dZ2, self.params['W2'].T)\n",
    "        dZ1 = dA1 * relu_derivative(self.cache['Z1'])\n",
    "        grads['dW1'] = np.dot(X.T, dZ1)\n",
    "        grads['db1'] = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key] -= learning_rate * grads['d' + key]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return cross_entropy_loss(y_true, y_pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# データの前処理\n",
    "def load_mnist_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1).astype(np.float32) / 255\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1).astype(np.float32) / 255\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# トレーニング関数\n",
    "def train(X, y, input_size, hidden_size1, hidden_size2, hidden_size3, output_size, epochs, batch_size, learning_rate):\n",
    "    net = FourLayerNet(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            y_pred = net.forward(X_batch)\n",
    "            loss = net.compute_loss(y_batch, y_pred)\n",
    "            grads = net.backward(X_batch, y_batch)\n",
    "            net.update_params(grads, learning_rate)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{epochs} - Loss: {loss:.4f}')\n",
    "\n",
    "    return net\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist_data()\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "hidden_size3 = 32\n",
    "output_size = 10\n",
    "epochs = 100\n",
    "batch_size = 2000\n",
    "learning_rate = 0.5\n",
    "\n",
    "net = train(X_train, y_train, input_size, hidden_size1, hidden_size2, hidden_size3, output_size, epochs, batch_size ,learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    0    1    1    1    4    0    1    3    0]\n",
      " [   0 1122    3    1    0    1    3    1    4    0]\n",
      " [   2    4  995   10    2    0    6    9    2    2]\n",
      " [   0    0   13  971    1   12    0    6    4    3]\n",
      " [   1    2    2    0  946    0    8    2    3   18]\n",
      " [   5    0    0   20    1  847    6    4    6    3]\n",
      " [   5    3    1    0    7    8  932    0    2    0]\n",
      " [   1    4    8    3    1    1    0 1002    1    7]\n",
      " [   3    1    5   13    3    9    6    5  924    5]\n",
      " [   6    7    2    7   15    4    1   10    6  951]]\n",
      "\n",
      "\n",
      "    Precision    Recall   F_value\n",
      "0   0.976815  0.988776  0.982759\n",
      "1   0.981627  0.988546  0.985075\n",
      "2   0.966019  0.964147  0.965082\n",
      "3   0.946394  0.961386  0.953831\n",
      "4   0.968270  0.963340  0.965799\n",
      "5   0.955982  0.949552  0.952756\n",
      "6   0.968815  0.972860  0.970833\n",
      "7   0.963462  0.974708  0.969052\n",
      "8   0.967539  0.948665  0.958009\n",
      "9   0.961577  0.942517  0.951952\n",
      "\n",
      "Accuracy: 0.9659\n"
     ]
    }
   ],
   "source": [
    "y_pre = net.predict(X_test)\n",
    "\n",
    "miss_count = 0\n",
    "conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "\n",
    "for i in range(len(y_pre)):\n",
    "    answer = np.argmax(y_test[i])\n",
    "    result = np.argmax(y_pre[i])\n",
    "    conf_matrix[answer][result] += 1\n",
    "    if answer != result: miss_count +=1\n",
    "    # print(answer, result, \"x\" if answer != result else \"\")\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "TP = np.array([conf_matrix[i][i] for i in range(10)])\n",
    "FN = np.array([np.sum(conf_matrix[i]) - conf_matrix[i][i] for i in range(10) ])\n",
    "FP = np.array([np.sum(conf_matrix.T[i]) - conf_matrix[i][i] for i in range(10) ])\n",
    "TN = np.array([np.sum(conf_matrix) - TP[i] - FN[i] - FP[i] for i in range(10)])\n",
    "print(conf_matrix)\n",
    "\n",
    "Precision = np.array([TP / (TP + FP)])\n",
    "Recall = np.array([TP / (TP + FN)])\n",
    "F_value = np.array([ 2 * Recall * Precision / (Recall + Precision)])\n",
    "\n",
    "df = pd.DataFrame({\"Precision\": Precision[0], \"Recall\": Recall[0], \"F_value\": F_value[0][0]})\n",
    "print(\"\\n\\n\", df)\n",
    "\n",
    "acc = (len(y_pre) - miss_count) / len(y_pre)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.49390477e-03 5.89338066e-04 2.91491985e-01 4.43597974e-02\n",
      "  1.23365806e-03 7.78064583e-02 2.46905910e-03 5.47577769e-01\n",
      "  1.59122691e-02 1.70657611e-02]\n",
      " [1.73087517e-03 9.41020522e-03 9.64326702e-01 7.98089237e-03\n",
      "  2.38574456e-04 1.29773299e-03 4.25021560e-03 7.46583155e-03\n",
      "  2.96731044e-03 3.31660245e-04]\n",
      " [1.41580238e-04 4.92007468e-04 1.47632573e-03 9.42676587e-01\n",
      "  5.65276016e-06 3.74485153e-02 4.57690266e-04 1.47745726e-03\n",
      "  8.02281962e-03 7.80136451e-03]\n",
      " [2.75010743e-03 5.53433621e-05 2.39541564e-02 8.91365616e-03\n",
      "  1.24928042e-03 2.83206457e-01 6.12551736e-01 9.35667379e-05\n",
      "  6.69561971e-02 2.69499246e-04]\n",
      " [4.59793257e-05 5.89944894e-07 4.72923605e-06 7.85358771e-03\n",
      "  3.11640829e-08 9.91468425e-01 6.41742427e-06 3.83102700e-06\n",
      "  1.14532828e-04 5.01875986e-04]\n",
      " [6.17141532e-04 2.42367277e-05 1.39934171e-04 4.39715035e-03\n",
      "  5.44285941e-05 9.68392015e-01 1.31199105e-03 5.69728421e-05\n",
      "  2.18022129e-02 3.20391682e-03]\n",
      " [8.69025984e-03 1.08366906e-02 7.24179627e-02 1.42359378e-01\n",
      "  3.06021348e-03 1.10443907e-01 3.58907819e-03 5.58818486e-01\n",
      "  3.09821679e-02 5.88018564e-02]\n",
      " [1.43529350e-01 2.98852703e-05 4.40479081e-02 7.50705268e-03\n",
      "  2.65214334e-05 1.43417959e-01 4.08775398e-01 1.10038801e-05\n",
      "  2.52635351e-01 1.95709897e-05]\n",
      " [1.29172866e-02 1.85713092e-02 3.54552676e-02 8.58974861e-02\n",
      "  1.30575271e-02 5.71435858e-01 5.86250264e-02 3.75129139e-02\n",
      "  1.21450200e-01 4.50771245e-02]]\n",
      "2.png 7\n",
      "2_1.png 2\n",
      "3.png 3\n",
      "4.png 6\n",
      "5.png 5\n",
      "6.png 5\n",
      "7.png 7\n",
      "7_1.png 6\n",
      "undefined.png 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "with os.scandir('./input') as entries:\n",
    "    items = [entry.name for entry in entries if entry.is_file()]\n",
    "\n",
    "data = []\n",
    "file_name = []\n",
    "\n",
    "for file in items:\n",
    "    if file == '.DS_Store':\n",
    "        continue\n",
    "    data.append(Image.open(f'./input/{file}').convert('L'))\n",
    "    file_name.append(file)\n",
    "\n",
    "data = np.array(data)\n",
    "x_pre = data.reshape(len(data), 784)\n",
    "x_pre = x_pre.astype('float32')\n",
    "x_pre /= 255\n",
    "y_pre = net.predict(x_pre)\n",
    "print(y_pre)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(file_name[i], np.argmax(y_pre[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
